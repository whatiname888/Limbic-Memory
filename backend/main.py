"""Minimal temporary FastAPI backend for Limbic Memory testing.

Endpoints are stubs so that the initialization script (start.sh) can
install dependencies and a simple server can be started for manual testing.

This file is intentionally lightweight and WILL be replaced by the real
memory engine implementation later.
"""
from __future__ import annotations

from fastapi import FastAPI, Request
from fastapi.responses import StreamingResponse, JSONResponse
from pydantic import BaseModel
from typing import List, Optional, AsyncGenerator
import time, json, os, asyncio, logging, uuid
from functools import lru_cache
import re

try:
    from openai import AsyncOpenAI  # type: ignore
except Exception:  # pragma: no cover
    AsyncOpenAI = None  # type: ignore
try:
    import chromadb  # type: ignore
    from chromadb.config import Settings as ChromaSettings  # type: ignore
except Exception:  # pragma: no cover
    chromadb = None  # type: ignore

VECTOR_DIR = os.path.join(os.path.dirname(__file__), "vector_db")
os.makedirs(VECTOR_DIR, exist_ok=True)
SHORT_MEM_DIR = os.path.join(os.path.dirname(__file__), "short_mem")
os.makedirs(SHORT_MEM_DIR, exist_ok=True)
SHORT_MEM_FILE = os.path.join(SHORT_MEM_DIR, "current.json")

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("limbic.backend")

app = FastAPI(title="Limbic Memory Backend", version="0.2.0")


class HealthResponse(BaseModel):
    status: str
    time: float


class ActivateRequest(BaseModel):
    query: str
    top_k: int = 5


class MemoryChunk(BaseModel):
    id: str
    content: str
    score: float
    source: Optional[str] = None


class ActivateResponse(BaseModel):
    query: str
    results: List[MemoryChunk]


class WriteRequest(BaseModel):
    chunks: List[str]


class WriteResponse(BaseModel):
    written: int
    ids: List[str] = []


# ----------------- Simple OpenAI Chat Support -----------------
class ChatMessage(BaseModel):
    role: str
    content: str

class ChatRequest(BaseModel):
    messages: List[ChatMessage]
    model: Optional[str] = None
    temperature: Optional[float] = None
    max_tokens: Optional[int] = None
    enable_memory_tools: bool = True  # æ˜¯å¦å¯ç”¨æ¨¡å‹é©±åŠ¨çš„è®°å¿†å·¥å…·è°ƒç”¨

CONFIG_PATH = os.path.join(os.path.dirname(__file__), "config.json")
CONFIG_EXAMPLE_PATH = os.path.join(os.path.dirname(__file__), "config.example.json")

@lru_cache(maxsize=1)
def load_config() -> dict:
    path = CONFIG_PATH if os.path.isfile(CONFIG_PATH) else CONFIG_EXAMPLE_PATH
    if not os.path.isfile(path):
        raise RuntimeError("Missing backend/config.json (copy config.example.json and set api_key)")
    data = json.loads(open(path, "r", encoding="utf-8").read())
    return data

def _build_client(cfg_section: dict, section_name: str):
    api_key = cfg_section.get("api_key", "")
    if not api_key or api_key.startswith("sk-REPLACE"):
        raise RuntimeError(f"{section_name} api_key not set in backend/config.json")
    if AsyncOpenAI is None:
        raise RuntimeError("openai package not installed")
    base_url = cfg_section.get("base_url") or None
    return AsyncOpenAI(api_key=api_key, base_url=base_url)

@lru_cache(maxsize=1)
def get_chat_client():
    cfg = load_config()
    # ä¼˜å…ˆ chat èŠ‚ç‚¹ï¼Œå‘åå…¼å®¹æ—§ openai èŠ‚ç‚¹
    chat_cfg = cfg.get("chat") or cfg.get("openai") or {}
    client = _build_client(chat_cfg, "chat")
    return client, chat_cfg

@lru_cache(maxsize=1)
def get_embedding_client():
    cfg = load_config()
    emb_cfg = cfg.get("embedding") or cfg.get("chat") or cfg.get("openai") or {}
    client = _build_client(emb_cfg, "embedding")
    return client, emb_cfg

@lru_cache(maxsize=1)
def get_embedding_dimension() -> int:
    cfg = load_config()
    emb_cfg = cfg.get("embedding") or {}
    dim = emb_cfg.get("dimension") or emb_cfg.get("dim") or 1536
    try:
        dim_int = int(dim)
        if dim_int <= 0:
            raise ValueError
        return dim_int
    except Exception:
        return 1536

# --------------- Hippocampus Client ---------------
@lru_cache(maxsize=1)
def get_hippocampus_client():
    cfg = load_config()
    hip_cfg = cfg.get("hippocampus") or {}
    if not hip_cfg:
        raise RuntimeError("hippocampus config missing")
    client = _build_client(hip_cfg, "hippocampus")
    return client, hip_cfg

def _load_short_mem() -> dict:
    if not os.path.isfile(SHORT_MEM_FILE):
        return {
            "version": 1,
            "background": "",
            "stm": [],  # å‹ç¼©åçš„å±‚çº§æˆ–æ¡ç›®
            "raw_history": [],
            "last_updated": time.time()
        }
    try:
        return json.loads(open(SHORT_MEM_FILE, 'r', encoding='utf-8').read())
    except Exception:
        return {"version":1, "background":"", "stm":[], "raw_history":[], "last_updated": time.time()}

def _save_short_mem(data: dict):
    data["last_updated"] = time.time()
    with open(SHORT_MEM_FILE, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def build_short_mem_context() -> Optional[str]:
    """ä»…æ„é€ éœ€è¦ä»¥ system å½¢å¼æ³¨å…¥çš„èƒŒæ™¯æ‘˜è¦æ–‡æœ¬ã€‚

    å˜æ›´: ä¸å†æŠŠæœ€è¿‘å¯¹è¯æ¡ç›®åˆå¹¶ä¸ºä¸€ä¸ª system å—ï¼›æœ€è¿‘åŸå§‹å¯¹è¯å°†ä»¥æ­£å¸¸ user/assistant è§’è‰²æ³¨å…¥ï¼Œé¿å…è¢«å…¨éƒ¨è§†ä½œç³»ç»ŸæŒ‡ä»¤ï¼Œä»è€Œæ›´è´´è¿‘çœŸå®å¯¹è¯è¯­å¢ƒã€‚
    è¿”å›å€¼: è‹¥æ—  background åˆ™è¿”å› Noneã€‚
    """
    try:
        cfg = load_config()
        hip = cfg.get("hippocampus", {})
        if not hip or not hip.get("inject_context", True):
            return None
        # ä»…è¿”å›èƒŒæ™¯æ‘˜è¦ï¼Œæ˜ç¡®è¿™æ˜¯â€œå¯¹ç”¨æˆ·çš„æè¿°â€è€Œéå¯¹æ¨¡å‹çš„å†è®¾å®šï¼Œé™ä½è§’è‰²æ··æ·†é£é™©
        prefix = hip.get("inject_prefix", "[ä¼šè¯èƒŒæ™¯æ‘˜è¦]")
        sm = _load_short_mem()
        background = (sm.get("background") or "").strip()
        if not background:
            return None
        disclaimer = "(è¯´æ˜: ä¸‹æ–¹ä¸ºå¯¹ã€ç”¨æˆ·ã€‘è¿‡å¾€å¯¹è¯çš„èšåˆæ‘˜è¦, ç”¨äºè¿ç»­æ€§å‚è€ƒã€‚å®ƒæè¿°çš„æ˜¯ç”¨æˆ·è€Œä¸æ˜¯ä½ æœ¬èº«, ä¸è¦æŠŠå…¶ä¸­ç¬¬ä¸€äººç§°å½“ä½œä½ è‡ªå·±çš„è¯; ä»…åœ¨éœ€è¦æ—¶ç”¨äºç†è§£ä¸æ¨ç†, ä¸è¦é€å¥å¤è¿°ç»™ç”¨æˆ·ã€‚)"
        lines = [prefix + " " + disclaimer, background[:600]]
        text = '\n'.join(lines)
        if len(text) > 3000:
            text = text[:2950] + '\n...(æˆªæ–­)'
        return text
    except Exception as e:
        logger.warning("build_short_mem_context failed: %s", e)
        return None

async def run_hippocampus_transform(new_user_text: str) -> dict:
    """è°ƒç”¨æµ·é©¬ä½“ LLMï¼Œå¯¹çŸ­æœŸè®°å¿†è¿›è¡Œç»“æ„åŒ–è½¬æ¢ï¼Œå¹¶è¿”å›æŠ¥å‘Šã€‚

    æœŸæœ›æ¨¡å‹è¾“å‡ºï¼ˆä»»æ„ä¸€ç§å³å¯è§£æï¼‰ï¼š
    ```json
    {"background": "...", "stm": [...], "ltm_candidates": ["..."]}
    ```
    è‹¥ç¼ºå¤±å­—æ®µä½¿ç”¨æ—§å€¼æˆ–ç©ºå€¼ã€‚
    """
    try:
        client, hip_cfg = get_hippocampus_client()
    except Exception as e:
        logger.info("hippocampus disabled: %s", e)
        return {"enabled": False}
    sm = _load_short_mem()
    # è¿½åŠ æœ€æ–°ç”¨æˆ·æ¶ˆæ¯ï¼ˆä¸å†åšä»»ä½•è£å‰ª/æˆªæ–­ï¼›å®Œæ•´çŸ­æœŸå¯¹è¯å®Œå…¨ç”±æµ·é©¬ä½“è´Ÿè´£è¯­ä¹‰æ•´åˆï¼Œä½†åŸå§‹ raw_history æ°¸ä¹…ä¿ç•™ï¼‰
    sm['raw_history'].append({"role": "user", "content": new_user_text, "ts": time.time()})
    enable_compression = hip_cfg.get("enable_compression", True)
    compression_prompt = hip_cfg.get("compression_prompt")
    # ä¸è£å‰ª, to_prune æ’ä¸ºç©º -> è‹¥æœªé…ç½®è‡ªå®šä¹‰ compression_prompt åˆ™ç›´æ¥è·³è¿‡å‹ç¼©
    to_prune: List[dict] = []
    system_prompt = hip_cfg.get("system_prompt") or (
        "ä½ æ˜¯æµ·é©¬ä½“è®°å¿†æ•´åˆæ¨¡å—ã€‚è¾“å…¥: æœ€è¿‘ç”¨æˆ·æ¶ˆæ¯ + ç°æœ‰ background + stm(å¯ä¸ºç©º)ã€‚è¾“å‡º JSON: {background, stm, ltm_candidates}.\n"
        "è§„åˆ™: background ä¸ºæ»šåŠ¨è¯­ä¹‰æ‘˜è¦(ä¸å«ç»†èŠ‚å¯¹è¯åŸæ–‡, ä½†ä¿ç•™é•¿æœŸäººç‰©/ç›®æ ‡/åå¥½/æƒ…ç»ªæ¨¡å¼); stm ä¸ºåˆ†å±‚æˆ–åˆ—è¡¨æ¡ç›®, æœ€æ–°å†…å®¹æ›´åŸå§‹; ltm_candidates ä¸ºéœ€å†™å…¥é•¿æœŸè®°å¿†çš„ç‹¬ç«‹æ–‡æœ¬æ•°ç»„, æ¯æ¡ç‹¬ç«‹å®Œæ•´è‡ªç„¶å¥ã€‚ä¸è¦è¿”å›é™¤ JSON ä»¥å¤–çš„å¤šä½™è¯´æ˜ã€‚"
    )
    # ç»„è£…è¾“å…¥æ¶ˆæ¯
    # æ„é€ ç»™å‹ç¼©æ¨¡å‹çš„è¾“å…¥ï¼šbackground + recent + to_pruneï¼ˆåªåœ¨æœ‰éœ€è¦æ—¶ï¼‰
    recent_dialog = []
    for r in sm.get('raw_history', [])[-min(len(sm.get('raw_history', [])), 20):]:  # æœ€è¿‘ 20 æ¡ç”¨äºä¸Šä¸‹æ–‡å‚è€ƒ
        if isinstance(r, dict) and r.get('role') in {"user","assistant"}:
            recent_dialog.append(f"{r['role']}: {r.get('content','')}")
    compression_input = {
        "background": sm.get("background", ""),
        "recent": recent_dialog,
        "to_prune": [f"{r.get('role')}: {r.get('content','')}" for r in to_prune if isinstance(r, dict) and r.get('role') in {"user","assistant"}]
    }
    if not enable_compression or (not to_prune and not compression_prompt):
        # ä¸è¿›è¡Œå‹ç¼©ï¼Œç›´æ¥ä¿å­˜å¹¶è¿”å›ï¼Œæ— æ–°å¢é•¿æœŸå†™å…¥
        _save_short_mem(sm)
        return {"enabled": True, "background": sm.get("background",""), "stm_size": len(sm.get("stm", [])), "ltm_written": [], "ltm_candidate_count": 0, "raw_output": ""}
    hip_messages = [
        {"role": "system", "content": compression_prompt or system_prompt},
        {"role": "user", "content": json.dumps(compression_input, ensure_ascii=False)}
    ]
    model = hip_cfg.get("model") or hip_cfg.get("fallback_model") or "gpt-4o-mini"
    temperature = hip_cfg.get("temperature", 0.2)
    try:
        resp = await client.chat.completions.create(  # type: ignore
            model=model,
            messages=hip_messages,
            temperature=temperature,
            stream=False
        )
        content = resp.choices[0].message.content if resp.choices else ""
    except Exception as e:
        logger.warning("hippocampus call failed: %s", e)
        return {"enabled": True, "error": str(e)}
    # è§£æ JSONï¼ˆæ”¯æŒ fenced codeï¼‰
    parsed = {}
    if content:
        m = re.search(r"```json\s*(\{[\s\S]+?\})\s*```", content)
        raw_json = None
        if m:
            raw_json = m.group(1)
        else:
            # å°è¯•ç›´æ¥æ•´ä½“è§£æ
            if content.strip().startswith('{') and content.strip().endswith('}'):
                raw_json = content.strip()
        if raw_json:
            try:
                parsed = json.loads(raw_json)
            except Exception:
                parsed = {}
    background_new = parsed.get('background') if isinstance(parsed.get('background'), str) else sm.get('background','')
    stm_new = sm.get('stm', [])  # ä¸å†æ›´æ–° stmï¼ˆé€æ­¥åºŸå¼ƒï¼‰
    ltm_candidates = parsed.get('ltm_candidates') if isinstance(parsed.get('ltm_candidates'), list) else []
    # å†™å…¥é•¿æœŸè®°å¿†ï¼ˆå‘é‡åº“ï¼‰
    written = []
    for cand in ltm_candidates:
        if not isinstance(cand, str) or not cand.strip():
            continue
        try:
            res = await tool_memory_store(cand.strip())
            written.append({"text": cand.strip()[:180], "id": res.get('id')})
        except Exception as e:
            written.append({"text": cand.strip()[:180], "error": str(e)})
    # æ›´æ–°çŸ­æœŸè®°å¿†æ–‡ä»¶
    sm['background'] = background_new
    sm['stm'] = stm_new  # ä¿ç•™å ä½ï¼Œæœªæ¥å¯ç§»é™¤
    _save_short_mem(sm)
    return {
        "enabled": True,
        "background": background_new,
        "stm_size": len(stm_new),
    "ltm_written": written,
    "ltm_candidate_count": len(ltm_candidates),
        "raw_output": content
    }


# --------------- Vector Memory (Chroma) ---------------
@lru_cache(maxsize=1)
def get_chroma():
    if chromadb is None:
        raise RuntimeError("chromadb not installed. Re-run install after adding requirement.")
    cfg = load_config()
    mem_cfg = cfg.get("memory", {})
    collection = mem_cfg.get("collection", "long_term_memory")
    client = chromadb.PersistentClient(path=VECTOR_DIR, settings=ChromaSettings(anonymized_telemetry=False))
    coll = client.get_or_create_collection(name=collection)
    return coll

async def embed_texts(texts: List[str]) -> List[List[float]]:
    # Use OpenAI embedding endpoint for simplicity; fallback to zeros if failure.
    try:
        client, cfg = get_embedding_client()
        # embedding èŠ‚ç‚¹: modelï¼›å…¼å®¹æ—§ chat/openai fallback
        root_cfg = load_config()
        model = cfg.get("model") or root_cfg.get("embedding_model") or "text-embedding-3-small"
        resp = await client.embeddings.create(model=model, input=texts)  # type: ignore
        return [d.embedding for d in resp.data]  # type: ignore
    except Exception as e:  # fallback deterministic
        logger.warning("embedding failed, fallback zeros: %s", e)
        dim = get_embedding_dimension()
        return [[0.0]*dim for _ in texts]

def format_tool_markdown(steps: List[dict]) -> str:
    if not steps:
        return ""
    # ç‰¹æ®Šå¤„ç†ï¼šé¦–æ¡ä¸ºè‡ªåŠ¨å¬å› memory_recall (auto) æ—¶å•ç‹¬ä»¥â€œè®°å¿†æ¿€æ´»â€æ ‡é¢˜å±•ç¤º
    first_auto = False
    if steps and isinstance(steps[0].get('name'), str) and 'memory_recall' in steps[0]['name'] and '(auto)' in steps[0]['name']:
        first_auto = True
    lines: List[str] = []
    if first_auto:
        s0 = steps[0]
        lines.append("### è®°å¿†æ¿€æ´»")
        lines.append("")
        # å±•ç¤ºè‡ªåŠ¨æ¿€æ´»çš„æŸ¥è¯¢å‚æ•°ä¸ç»“æœæ‘˜è¦
        lines.append("**æ¥æº**: è‡ªåŠ¨æ£€ç´¢ (auto_query_recall)")
        if 'arguments' in s0:
            try:
                lines.append(f"- æŸ¥è¯¢: `{json.dumps(s0['arguments'], ensure_ascii=False)[:280]}`")
            except Exception:
                pass
        if 'result_preview' in s0:
            lines.append(f"- ç»“æœ: {s0['result_preview']}")
        if 'duration_ms' in s0:
            lines.append(f"- è€—æ—¶: {s0['duration_ms']} ms")
        lines.append("")
        remaining = steps[1:]
        if remaining:
            lines.append("### å·¥å…·è°ƒç”¨æ­¥éª¤ (æ¨¡å‹çœŸå®è§¦å‘)")
            lines.append("")
            for idx, s in enumerate(remaining, 1):
                lines.append(f"**æ­¥éª¤ {idx}: {s.get('name','tool')}**")
                if 'arguments' in s:
                    try:
                        lines.append(f"- å‚æ•°: `{json.dumps(s['arguments'], ensure_ascii=False)[:280]}`")
                    except Exception:
                        pass
                if 'result_preview' in s:
                    lines.append(f"- è¿”å›: {s['result_preview']}")
                if 'duration_ms' in s:
                    lines.append(f"- è€—æ—¶: {s['duration_ms']} ms")
                lines.append("")
    else:
        lines.append("### å·¥å…·è°ƒç”¨æ­¥éª¤ (æ¨¡å‹çœŸå®è§¦å‘)")
        lines.append("")
        for i, s in enumerate(steps, 1):
            lines.append(f"**æ­¥éª¤ {i}: {s.get('name','tool')}**")
            if 'arguments' in s:
                try:
                    lines.append(f"- å‚æ•°: `{json.dumps(s['arguments'], ensure_ascii=False)[:280]}`")
                except Exception:
                    pass
            if 'result_preview' in s:
                lines.append(f"- è¿”å›: {s['result_preview']}")
            if 'duration_ms' in s:
                lines.append(f"- è€—æ—¶: {s['duration_ms']} ms")
            lines.append("")
    lines.append("---\n")
    return "\n".join(lines)

# --------- Memory Tools ---------
async def tool_memory_store(text: str) -> dict:
    try:
        coll = get_chroma()
        emb = await embed_texts([text])
        mid = str(uuid.uuid4())
        coll.add(ids=[mid], documents=[text], embeddings=emb)
        return {"id": mid, "length": len(text)}
    except Exception as e:
        return {"error": str(e)}

async def tool_memory_recall(query: str, top_k: Optional[int] = None) -> dict:
    try:
        cfg = load_config()
        mem_cfg = cfg.get("memory", {})
        k = top_k or mem_cfg.get("top_k", 5)
        coll = get_chroma()
        emb = await embed_texts([query])
        res = coll.query(query_embeddings=emb, n_results=k)
        docs = res.get('documents') or []
        ids = res.get('ids') or []
        flat = []
        for gi, group in enumerate(docs):
            for ji, doc in enumerate(group):
                _id = ids[gi][ji] if gi < len(ids) and ji < len(ids[gi]) else ""
                flat.append({"id": _id, "content": doc})
        return {"results": flat[:k], "count": len(flat)}
    except Exception as e:
        return {"error": str(e)}

def build_tool_spec():
    return [
        {"type": "function", "function": {"name": "memory_store", "description": "å­˜å‚¨æ–°çš„é•¿æœŸè®°å¿†æ–‡æœ¬", "parameters": {"type": "object", "properties": {"text": {"type": "string"}}, "required": ["text"]}}},
        {"type": "function", "function": {"name": "memory_recall", "description": "æ£€ç´¢ç›¸å…³è®°å¿†", "parameters": {"type": "object", "properties": {"query": {"type": "string"}, "top_k": {"type": "integer", "default": 5}}, "required": ["query"]}}}
    ]

INLINE_TOOL_PATTERN = re.compile(r"^\s*\{\s*\"name\"\s*:\s*\"(memory_store|memory_recall)\"", re.IGNORECASE)

def _try_parse_inline_tool(content: str):
    if not content or not INLINE_TOOL_PATTERN.search(content):
        return None
    try:
        data = json.loads(content)
        name = data.get("name")
        args = data.get("arguments") or {}
        if name in {"memory_store", "memory_recall"}:
            return name, args
    except Exception:
        return None
    return None

async def run_tool_iteration(client, model_name: str, messages, tools):
    executed: List[dict] = []
    cfg = load_config()
    mem_cfg = cfg.get("memory", {})
    loops_cfg = mem_cfg.get("max_tool_loops", 8)
    # å®‰å…¨ä¸Šé™é˜²æ­¢æ­»å¾ªç¯
    loops = min(int(loops_cfg) if isinstance(loops_cfg, (int, float, str)) and str(loops_cfg).isdigit() else 8, 30)
    for loop_idx in range(loops):
        try:
            resp = await client.chat.completions.create(  # type: ignore
                model=model_name,
                messages=messages,
                tools=tools,
                tool_choice="auto",
                temperature=0.2,
                stream=False,
            )
        except Exception as e:
            logger.warning("tool planning call failed(loop %s): %s", loop_idx, e)
            break
        choice = resp.choices[0]
        msg = choice.message
        tool_calls = getattr(msg, 'tool_calls', None)
        inline_call = None
        if not tool_calls and getattr(msg, 'content', None):
            inline_call = _try_parse_inline_tool(msg.content)
        if not tool_calls and not inline_call:
            # æ²¡æœ‰æ›´å¤šå·¥å…·è°ƒç”¨ -> è¿½åŠ æœ€ç»ˆ assistant å†…å®¹ï¼ˆè‡ªç„¶è¯­è¨€ï¼‰
            if getattr(msg, 'content', None):
                messages.append({"role": "assistant", "content": msg.content})
            break
        # è®°å½•æ¨¡å‹å†³å®šè°ƒç”¨çš„å·¥å…·ï¼ˆassistant messageï¼‰
        if tool_calls:
            messages.append({
                "role": "assistant",
                "content": msg.content or "",
                "tool_calls": [
                    {"id": tc.id, "type": tc.type, "function": {"name": tc.function.name, "arguments": tc.function.arguments}}
                    for tc in tool_calls
                ]
            })
        elif inline_call:
            # æ¨¡å‹ç›´æ¥è¾“å‡º JSON å½¢å¼ {"name":"memory_store", "arguments":{...}}
            name, arguments = inline_call
            # æ„é€ ä¼ª tool_call id ä»¥ç»Ÿä¸€åç»­å¤„ç†
            fake_id = str(uuid.uuid4())
            messages.append({
                "role": "assistant",
                "content": "",  # ä¸æŠŠ JSON åŸæ ·ç•™ç»™åç»­å›ç­”
                "tool_calls": [{
                    "id": fake_id,
                    "type": "function",
                    "function": {"name": name, "arguments": json.dumps(arguments, ensure_ascii=False)}
                }]
            })
            # æ„é€ ä¸€ä¸ªæ¨¡æ‹Ÿçš„å¯¹è±¡æ¥å£ç»“æ„
            class FakeFunc:  # minimal shim
                def __init__(self, name, arguments):
                    self.name = name
                    self.arguments = json.dumps(arguments, ensure_ascii=False)
            class FakeTC:
                def __init__(self, _id, name, args):
                    self.id = _id
                    self.type = 'function'
                    self.function = FakeFunc(name, args)
            tool_calls = [FakeTC(fake_id, name, arguments)]
        # é€ä¸ªæ‰§è¡Œå·¥å…·
        for tc in tool_calls:
            name = tc.function.name
            raw_args = tc.function.arguments or '{}'
            try:
                parsed = json.loads(raw_args)
            except Exception:
                parsed = {}
            t0 = time.time()
            if name == 'memory_store':
                result = await tool_memory_store(parsed.get('text',''))
            elif name == 'memory_recall':
                result = await tool_memory_recall(parsed.get('query',''), parsed.get('top_k'))
            else:
                result = {"error": f"unknown tool {name}"}
            dur = int((time.time() - t0) * 1000)
            preview = json.dumps(result, ensure_ascii=False)[:500]
            executed.append({
                "name": name,
                "arguments": parsed,
                "result_preview": preview,
                "duration_ms": dur
            })
            # å·¥å…·ç»“æœæ¶ˆæ¯ï¼ˆæ¨¡å‹ä¸‹ä¸€è½®å¯è¯»å–ï¼‰
            messages.append({
                "role": "tool",
                "tool_call_id": tc.id,
                "name": name,
                "content": json.dumps(result, ensure_ascii=False)
            })
    return messages, executed


@app.get("/healthz", response_model=HealthResponse, summary="Health check")
def healthz():
    return HealthResponse(status="ok", time=time.time())

# å…¼å®¹ /health (æœ‰äº›åå‘ä»£ç†æˆ–ç›‘æ§é»˜è®¤æ¢æ´»è·¯å¾„) ä¸å†™å…¥ OpenAPI æ–‡æ¡£
@app.get("/health", include_in_schema=False)
def health_alias():
    return healthz()


@app.post("/chat/stream")
async def chat_stream(body: ChatRequest, request: Request):
    """OpenAI ç›´è¿æµå¼æ¥å£ï¼Œç¬¦åˆå‰ç«¯ /api/chat è½¬å‘é¢„æœŸã€‚

    è¿”å› SSE é£æ ¼ï¼šdata: {json}\n  æœ«å°¾ data: [DONE]\n
    """
    logger.info("/chat/stream request: messages=%d model=%s", len(body.messages), body.model or "<default>")
    try:
        client, ocfg = get_chat_client()
    except Exception as e:
        logger.error("config/openai init error: %s", e)
        return JSONResponse({"error": str(e)}, status_code=500)

    # æ¨¡å‹åå½’ä¸€åŒ–ï¼šå‰ç«¯å¦‚æœä¼ äº† JSON schema é»˜è®¤ "string" / "model" / ç©ºç­‰å ä½ï¼Œå›é€€åˆ°é…ç½®é»˜è®¤
    invalid_placeholders = {None, "", "string", "model", "undefined", "null"}
    raw_model = (body.model or "").strip() if body.model else ""
    if raw_model.lower() in invalid_placeholders:
        model = ocfg.get("model", "gpt-4o-mini")
        logger.info("model placeholder '%s' -> fallback '%s'", raw_model or "<empty>", model)
    else:
        model = raw_model
    temperature = body.temperature if body.temperature is not None else ocfg.get("temperature", 0.7)
    max_tokens = body.max_tokens if body.max_tokens is not None else ocfg.get("max_output_tokens") or None

    # Normalize roles & prepend configurable system prompt and injected memory/background
    msgs = []
    base_system = ocfg.get("system_prompt")
    if base_system:
        msgs.append({"role": "system", "content": base_system})
    # ä»…ä»¥ system æ³¨å…¥èƒŒæ™¯æ‘˜è¦
    sm_background = build_short_mem_context()
    if sm_background:
        msgs.append({"role": "system", "content": sm_background})
    # ä»¥æ­£å¸¸ user/assistant è§’è‰²æ³¨å…¥å…¨éƒ¨å½“å‰çŸ­æœŸè®°å¿† raw_historyï¼ˆæµ·é©¬ä½“è´Ÿè´£é•¿åº¦æ§åˆ¶ï¼‰
    try:
        sm_all = _load_short_mem()
        rh_all = sm_all.get("raw_history") or []
        seq = [r for r in rh_all if isinstance(r, dict) and r.get("role") in {"user","assistant"} and r.get("content")]
        existing_pairs = {(m.role, m.content) for m in body.messages}
        injected = 0
        injected_raw_indices = []  # è®°å½•ç´¢å¼•ç”¨äºåç»­é¿å…è¢« max_history æˆªæ–­
        for r in seq:
            tup = (r.get("role"), r.get("content"))
            if tup in existing_pairs:
                continue
            msgs.append({"role": r.get("role"), "content": r.get("content"), "_injected_raw": True})
            injected_raw_indices.append(len(msgs)-1)
            injected += 1
        logger.info("[raw-history-inject] mode=ALL injected=%d total_raw=%d", injected, len(seq))
        if seq:
            logger.info("[raw-history-first] role=%s len=%d preview=%s", seq[0].get('role'), len(seq[0].get('content','')), seq[0].get('content','')[:80].replace('\n',' '))
    except Exception as inj_e:
        logger.warning("inject raw history messages failed: %s", inj_e)
    if body.enable_memory_tools:
        tool_instruction = (
            "[è®°å¿†å·¥å…·ç²¾ç®€æŒ‡ä»¤]\n"
            "å·¥å…·: memory_recall(query, top_k?), memory_store(text)\n"
            "ä¼˜å…ˆ: å‡ ä¹æ¯æ¬¡ç­”å¤å‰å…ˆ memory_recallï¼›è‹¥æ— ç»“æœ -> ç®€çŸ­è¯´æ˜æš‚æ— è®°å½•å¹¶é‚€è¯·è¡¥å……ã€‚\n"
            "å­˜å‚¨è§¦å‘: å‘ç°æ–°çš„ä¸”å¯èƒ½åœ¨åç»­ä»æœ‰ä»·å€¼çš„ â€˜èº«ä»½/åå¥½/é•¿æœŸç›®æ ‡/æŒç»­æƒ…ç»ªæˆ–è¡Œä¸ºæ¨¡å¼/ä»ªå¼ä¹ æƒ¯â€™ å°±è°ƒç”¨ memory_storeï¼›ä¸€æ¬¡ç”¨æˆ·è¾“å…¥å¯å¤šæ¡ã€‚\n"
            "å­˜å‚¨æ ¼å¼: è‡ªç„¶è¯­è¨€å®Œæ•´å¥å¼ (æ—¥è®°é£æ ¼)ï¼ŒåŒ…å«ä¸»ä½“+è¦ç‚¹+åŠ¨æœº/èƒŒæ™¯ï¼Œä¸å†™ç¢ç‰‡ã€‚\n"
            "ç¦æ­¢: å›å¤é‡Œç›´æ¥å‡ºç° memory_recall(...) / memory_store(...) æˆ– JSONï¼›å·¥å…·åªåœ¨å†…éƒ¨ã€‚\n"
            "ç¨³å¥: ç»ä¸è‡†é€ ç”¨æˆ·æœªè¡¨è¾¾çš„äº‹å®ï¼›ä¸ç¡®å®šå°±å…ˆé—®ï¼Œå†å­˜ã€‚\n"
            "å›ç­”æ•´åˆ: å·¥å…·åç»™ç”¨æˆ·è‡ªç„¶è¯­è¨€å…±æƒ…+æ•´åˆï¼Œå¿…è¦æ—¶å¼•ç”¨å·²çŸ¥è®°å¿†è¦ç‚¹ã€‚\n"
            "é¼“åŠ±: è‹¥çŠ¹è±«æ˜¯å¦å­˜ä¸”ä¿¡æ¯å…·é•¿æœŸç‰¹å¾ -> å€¾å‘äºå­˜ã€‚"
        )
        msgs.append({"role": "system", "content": tool_instruction})
    for m in body.messages:
        role = m.role if m.role in {"user", "assistant", "system"} else "user"
        msgs.append({"role": role, "content": m.content})
    # é™åˆ¶å¯¹è¯å†å²æ¡æ•°ï¼ˆä¸å«æˆ‘ä»¬åœ¨ä¸Šé¢æ’å…¥çš„ system æ³¨å…¥ï¼‰ï¼Œåªè£å‰ªç”¨æˆ·/assistant/systemçš„åŸå§‹å¯¹è¯éƒ¨åˆ†
    try:
        max_hist = int(ocfg.get("max_history_messages", 0) or 0)
        if max_hist > 0:
            # è‹¥å­˜åœ¨æ³¨å…¥çš„ raw å†å²ï¼Œåˆ™è·³è¿‡è£å‰ªï¼Œé˜²æ­¢æ„å¤–åˆ é™¤ã€‚å¯é€šè¿‡è®¾ç½® max_history_messages<=0 æˆ–åç»­æ–°å¢é…ç½®æ§åˆ¶ã€‚
            has_injected = any(isinstance(m, dict) and m.get("_injected_raw") for m in msgs)
            if has_injected:
                logger.info("[trim-skip] detected injected raw history -> skip max_history_messages=%s trimming", max_hist)
            else:
                prefix_count = 0
                for mm in msgs:
                    if mm["role"] == "system" and (mm["content"].startswith("[è®°å¿†å·¥å…·ç²¾ç®€æŒ‡ä»¤]") or mm["content"].startswith("I. æ ¸å¿ƒèº«ä»½") or mm["content"].startswith("[çŸ­æœŸè®°å¿†æ³¨å…¥") or mm["content"].startswith("[çŸ­æœŸè®°å¿†æ³¨å…¥]")):
                        prefix_count += 1
                    else:
                        break
                head = msgs[:prefix_count]
                rest = msgs[prefix_count:]
                if len(rest) > max_hist:
                    rest = rest[-max_hist:]
                msgs = head + rest
    except Exception as e:
        logger.warning("trim history failed: %s", e)

    # ---------- ä¸Šä¸‹æ–‡é¢„ç®—æ§åˆ¶ï¼ˆé˜²æ­¢æä¾›å•†è‡ªèº«è£å‰ªå¯¼è‡´æœ€æ—©æ¶ˆæ¯ä¸¢å¤±ï¼‰ ----------
    def enforce_context_budget(messages: list):
        try:
            cfg_all = load_config()
            hip_cfg = cfg_all.get("hippocampus", {}) if isinstance(cfg_all, dict) else {}
            budget_chars = int(hip_cfg.get("inject_char_budget", 0) or 0)
            if budget_chars <= 0:
                return messages  # ä¸å¯ç”¨é¢„ç®—
            # ä»…ç»Ÿè®¡ user/assistant æ³¨å…¥å†…å®¹(å« _injected_raw) + åŸå§‹å¯¹è¯ï¼Œä¸åŒ…å« system æŒ‡ä»¤
            # æ‰¾å‡ºæŒ‰å‡ºç°é¡ºåºçš„ injected_raw åˆ—è¡¨ç´¢å¼•
            injected_indices = [i for i,m in enumerate(messages) if isinstance(m, dict) and m.get("_injected_raw")]
            if not injected_indices:
                return messages
            # pin æœ€æ—© N æ¡
            pin_first = int(hip_cfg.get("inject_pin_first", 2) or 2)
            pin_first = max(0, min(pin_first, len(injected_indices)))
            total_chars = 0
            per_index_len = {}
            for i,m in enumerate(messages):
                if m.get("role") in {"user","assistant"}:
                    l = len(m.get("content") or "")
                    total_chars += l
                    per_index_len[i] = l
            if total_chars <= budget_chars:
                logger.info("[budget] total_chars=%d <= budget=%d (no trim)", total_chars, budget_chars)
                return messages
            # ä¿ç•™ç­–ç•¥ï¼šä¿ç•™ pinned earliest injected_raw + æ‰€æœ‰è¿‘æœŸå¯¹è¯(å€’åºç´¯è®¡ç›´åˆ°æ»¡è¶³é¢„ç®—)ï¼›åˆ é™¤ä¸­é—´å¤šä½™ injected_raw
            # å…ˆé€‰å‡ºå¿…é¡»ä¿ç•™é›†åˆ
            pinned_set = set(injected_indices[:pin_first])
            # ä»ç»“å°¾å‘å‰ç´¯è®¡ç›´åˆ°é¢„ç®—æ»¡è¶³ï¼ˆåŒ…å«æ‰€æœ‰ system + pinnedï¼‰
            keep_set = set(pinned_set)
            running = 0
            # é¢„å…ˆåŠ ä¸Š system æ¶ˆæ¯é•¿åº¦ï¼ˆéƒ½ä¿ç•™ï¼‰
            for i,m in enumerate(messages):
                if m.get("role") == "system":
                    running += len(m.get("content") or "")
                    keep_set.add(i)
            # å€’åºéå†ï¼Œä¿ç•™æœ€è¿‘æ¶ˆæ¯
            for i in range(len(messages)-1, -1, -1):
                if i in keep_set:
                    continue
                m = messages[i]
                if m.get("role") in {"user","assistant"}:
                    l = per_index_len.get(i, len(m.get("content") or ""))
                    if running + l > budget_chars and i not in pinned_set:
                        continue  # ä¸å†åŠ å…¥
                    keep_set.add(i)
                    running += l
                else:
                    keep_set.add(i)
            # éœ€è¦åˆ é™¤çš„ injected_raw å³é‚£äº› _injected_raw ä¸”ä¸åœ¨ keep_set
            removed_indices = [i for i in injected_indices if i not in keep_set]
            if not removed_indices:
                logger.info("[budget] after selection still within budget running=%d budget=%d", running, budget_chars)
                return messages
            # æ±‡æ€»è¢«åˆ å†…å®¹é•¿åº¦å’Œæ¡æ•°ï¼Œæ„é€ ä¸€ä¸ªå‹ç¼©æ‘˜è¦æ¶ˆæ¯æ’åœ¨ pinned æœ€åä¸€ä¸ªä¹‹å
            removed_chars = sum(per_index_len.get(i,0) for i in removed_indices)
            removed_count = len(removed_indices)
            first_insert_pos = max(pinned_set) + 1 if pinned_set else 0
            # æ„é€ æ‘˜è¦ï¼ˆç®€å•ï¼‰
            summary_text = f"[è®°å¿†ä¸­é—´æ®µå·²å‹ç¼© {removed_count} æ¡ ~{removed_chars}å­— å·²æ±‡æ€»äº background]"
            compress_msg = {"role": "system", "content": summary_text, "_compressed_gap": True}
            new_messages = []
            for idx,m in enumerate(messages):
                if idx == first_insert_pos:
                    new_messages.append(compress_msg)
                if idx in removed_indices:
                    continue
                new_messages.append(m)
            logger.info("[budget-trim] removed=%d chars=%d pinned=%d final_len=%d", removed_count, removed_chars, len(pinned_set), len(new_messages))
            return new_messages
        except Exception as be:
            logger.warning("context budget enforcement failed: %s", be)
            return messages

    # åº”ç”¨é¢„ç®—ï¼ˆä»…å½“é…ç½®æ˜¾å¼å¼€å¯ enable_context_budget æ‰ä¼šå‹ç¼©ï¼›é»˜è®¤è·³è¿‡ç¡®ä¿ raw_history åŸå°ä¸åŠ¨ï¼‰
    if ocfg.get("enable_context_budget", False):
        msgs = enforce_context_budget(msgs)
    else:
        logger.info("[budget-skip] enable_context_budget not set -> forwarding full raw_history count=%d", sum(1 for m in msgs if m.get('_injected_raw')))

    executed_steps: List[dict] = []
    # é¢„å…ˆï¼šå°è¯•è°ƒç”¨æµ·é©¬ä½“æ¨¡å—ï¼ˆä½¿ç”¨æœ€æ–°ä¸€æ¡ç”¨æˆ·æ¶ˆæ¯ï¼‰æ„å»ºçŸ­æœŸè®°å¿†ï¼ˆä¸é˜»å¡å·¥å…·è°ƒç”¨; å¤±è´¥å¿½ç•¥ï¼‰
    latest_user_text = ""
    for m in reversed(body.messages):
        if m.role == 'user' and m.content:
            latest_user_text = m.content
            break
    # å¹¶è¡Œå¯åŠ¨æµ·é©¬ä½“ä»»åŠ¡ï¼ˆé¿å…é˜»å¡ä¸»æ¨¡å‹æ€è€ƒï¼‰ï¼Œç¨ååœ¨è¾“å‡ºæœ«å°¾ await
    hippocampus_report: dict = {}
    hippocampus_task: Optional[asyncio.Task] = None
    if latest_user_text:
        try:
            hippocampus_task = asyncio.create_task(run_hippocampus_transform(latest_user_text))
        except Exception as e:
            hippocampus_report = {"enabled": False, "error": str(e)}
    else:
        hippocampus_report = {"enabled": False, "error": "no_latest_user_message"}
    if body.enable_memory_tools:
        try:
            tools = build_tool_spec()
            msgs, executed_steps = await run_tool_iteration(client, model, msgs, tools)
            # å¦‚æœæ²¡æœ‰ä»»ä½• recall æ‰§è¡Œä¸”é…ç½® auto_query_recall ä¸º trueï¼Œåˆ™åŸºäºæœ€è¿‘ç”¨æˆ·è¯­å¥è‡ªåŠ¨è§¦å‘ä¸€æ¬¡ recall
            try:
                cfg_all = load_config()
                force_recall = cfg_all.get("memory",{}).get("auto_query_recall", False)
            except Exception:
                force_recall = False
            has_recall = any(s.get("name") == "memory_recall" for s in executed_steps)
            if force_recall and not has_recall:
                cfg_mem = cfg_all.get("memory", {}) if 'cfg_all' in locals() else {}
                q_max_chars = int(cfg_mem.get("auto_query_recall_query_max_chars", 800) or 800)
                q_top_k = int(cfg_mem.get("auto_query_recall_top_k", cfg_mem.get("top_k",5)) or 5)
                # ä»¥æœ€è¿‘ä¸€æ¡ç”¨æˆ·æ¶ˆæ¯å†…å®¹ä½œä¸º query
                user_query = ""
                for m in reversed(msgs):
                    if m.get("role") == "user" and m.get("content"):
                        content = m["content"].strip()
                        user_query = content[-q_max_chars:]
                        break
                if user_query.strip():
                    t0 = time.time()
                    recall_result = await tool_memory_recall(user_query, q_top_k)
                    dur = int((time.time()-t0)*1000)
                    executed_steps.append({
                        "name": "memory_recall (auto)",
                        "arguments": {"query": user_query[:200], "top_k": q_top_k},
                        "result_preview": json.dumps(recall_result, ensure_ascii=False)[:500],
                        "duration_ms": dur
                    })
                    fake_id = str(uuid.uuid4())
                    msgs.append({"role": "assistant", "content": "", "tool_calls": [{"id": fake_id, "type": "function", "function": {"name": "memory_recall", "arguments": json.dumps({"query": user_query, "top_k": q_top_k}, ensure_ascii=False)}}]})
                    msgs.append({"role": "tool", "tool_call_id": fake_id, "name": "memory_recall", "content": json.dumps(recall_result, ensure_ascii=False)})
            # ç»“æŸåå¦‚æœæ²¡æœ‰ store è¡Œä¸ºä½†å‡ºç°æ˜æ˜¾å¯å­˜ä¿¡æ¯ï¼ˆå¯å‘å¼ï¼‰ï¼šç”¨æˆ·æ¶ˆæ¯åŒ…å« å–œæ¬¢/çˆ±/ç›®æ ‡/æƒ³/è®¡åˆ’/ä¹ æƒ¯
            has_store = any(s.get("name") == "memory_store" for s in executed_steps)
            if not has_store:
                # å–æœ€è¿‘ç”¨æˆ·æ¶ˆæ¯ï¼Œåšä¸€ä¸ªç®€å•å¯å‘å¼
                latest_user = ""
                for m in reversed(msgs):
                    if m.get("role") == "user" and m.get("content"):
                        latest_user = m["content"]
                        break
                if latest_user:
                    if re.search(r"(å–œæ¬¢|çˆ±|ç›®æ ‡|æƒ³è¦|æƒ³åš|è®¡åˆ’|ä¹ æƒ¯|åå¥½)", latest_user):
                        # æ„é€ å€™é€‰å­˜å‚¨æ–‡æœ¬ï¼ˆå‹ç¼©åˆ° 200 å­—ï¼‰
                        candidate = latest_user.strip().replace("\n"," ")
                        candidate = candidate[:200]
                        if candidate:
                            t0 = time.time()
                            store_result = await tool_memory_store(candidate)
                            dur = int((time.time()-t0)*1000)
                            executed_steps.append({
                                "name": "memory_store",
                                "arguments": {"text": candidate[:120]},
                                "result_preview": json.dumps(store_result, ensure_ascii=False)[:300],
                                "duration_ms": dur
                            })
                            fake_id2 = str(uuid.uuid4())
                            msgs.append({"role": "assistant", "content": "", "tool_calls": [{"id": fake_id2, "type": "function", "function": {"name": "memory_store", "arguments": json.dumps({"text": candidate}, ensure_ascii=False)}}]})
                            msgs.append({"role": "tool", "tool_call_id": fake_id2, "name": "memory_store", "content": json.dumps(store_result, ensure_ascii=False)})
            # åˆ¤æ–­æ˜¯å¦éœ€è¦è¡¥ä¸€ä¸ªæ˜ç¡®çš„è‡ªç„¶è¯­è¨€ç­”å¤
            if msgs:
                last = msgs[-1]
                need_final = False
                # æƒ…å†µ1ï¼šæœ€åæ˜¯ tool ç»“æœ
                if last["role"] == "tool":
                    need_final = True
                # æƒ…å†µ2ï¼šæœ€å assistant ä½† content ä¸ºç©ºæˆ–åªæœ‰ç©ºç™½
                elif last["role"] == "assistant" and (not last.get("content") or not last["content"].strip()):
                    need_final = True
                # æƒ…å†µ3ï¼šæœ€å assistant ä¸”åŒ…å« tool_calls ä½†ç¼ºå°‘è‡ªç„¶è¯­è¨€è§£é‡Š
                elif last["role"] == "assistant" and last.get("tool_calls") and (not last.get("content")):
                    need_final = True
                if need_final:
                    # ç»™æ¨¡å‹ä¸€ä¸ªæ˜ç¡®æŒ‡ä»¤ï¼šåŸºäºå·¥å…·ç»“æœç»§ç»­è‡ªç„¶è¯­è¨€å›å¤
                    msgs.append({
                        "role": "system",
                        "content": "ä»¥ä¸Šæ˜¯å†…éƒ¨è®°å¿†å·¥å…·è°ƒç”¨ç»“æœã€‚è¯·é¢å‘ç”¨æˆ·ç»™å‡ºè‡ªç„¶è¯­è¨€å›åº”ï¼Œèåˆå¿…è¦è®°å¿†ï¼Œä¸è¦å†è¾“å‡º JSONã€å‡½æ•°æˆ–å·¥å…·åç§°ã€‚"
                    })
                    try:
                        final_resp = await client.chat.completions.create(  # type: ignore
                            model=model,
                            messages=msgs,
                            temperature=temperature,
                            stream=False
                        )
                        fr_msg = final_resp.choices[0].message
                        if getattr(fr_msg, 'content', None):
                            msgs.append({"role": "assistant", "content": fr_msg.content})
                    except Exception as fe:
                        logger.warning("final natural language fill failed: %s", fe)
        except Exception as e:
            logger.warning("memory tool iteration failed: %s", e)

    async def gen() -> AsyncGenerator[bytes, None]:
        # ç«‹å³å‘é€ä¸€ä¸ªèµ·å§‹ç©ºå¢é‡ï¼Œé¿å…å‰ç«¯é•¿æ—¶é—´æ— å­—èŠ‚è¯¯åˆ¤è¶…æ—¶ / ç§»é™¤å›¾æ ‡
        start_payload = {"choices": [{"delta": {"content": ""}}], "__start": True}
        yield f"data: {json.dumps(start_payload, ensure_ascii=False)}\n".encode("utf-8")
        # å…ˆè¾“å‡ºå·¥å…·æ­¥éª¤ markdown
        md = format_tool_markdown(executed_steps)
        if md:
            for line in md.split('\n'):
                if not line:
                    continue
                payload = {"choices":[{"delta":{"content": line + "\n"}}]}
                yield f"data: {json.dumps(payload, ensure_ascii=False)}\n".encode("utf-8")
        # æ„é€ æµ·é©¬ä½“æ€»ç»“ä¸²ï¼ˆåµŒå…¥åˆ°åŒä¸€æ¡ assistant è¾“å‡ºæœ«å°¾ï¼Œè€Œä¸æ˜¯ç‹¬ç«‹ markdown åŒºå—ï¼‰
        async def finalize_and_build_hippocampus_summary() -> str:
            nonlocal hippocampus_report
            if hippocampus_task is not None:
                if not hippocampus_task.done():
                    try:
                        hippocampus_report = await asyncio.wait_for(hippocampus_task, timeout=6.0)
                    except asyncio.TimeoutError:
                        hippocampus_report = {"enabled": False, "error": "timeout"}
                    except Exception as he:
                        hippocampus_report = {"enabled": False, "error": str(he)}
                else:
                    try:
                        hippocampus_report = hippocampus_task.result()
                    except Exception as he2:
                        hippocampus_report = {"enabled": False, "error": str(he2)}
            elif not hippocampus_report:
                hippocampus_report = {"enabled": False, "error": "not_started"}
            rep = hippocampus_report or {}
            # æ›´æ¸…æ™°çš„å¤šè¡Œå—æ ¼å¼ï¼Œä»åµŒå…¥åŒä¸€å›ç­”æµ
            status_msg = "OK" if rep.get("enabled") else (rep.get("error") or "disabled")
            bg_line = "(æ— )"
            if rep.get("background"):
                bg_line = rep['background'][:160].replace('\n', ' ')
            written_any = rep.get('ltm_written') or []
            write_lines = []
            for w in written_any[:3]:
                if w.get('id'):
                    write_lines.append(f"  - âœ… {w['id']}: {w['text']}")
                else:
                    write_lines.append(f"  - âŒ {w.get('text')}: {w.get('error')}")
            if not write_lines:
                write_lines.append("  - (æ— )")
            ltm_cnt = rep.get('ltm_candidate_count')
            stm_sz = rep.get('stm_size')
            header = "---\nğŸ§  è®°å¿†æ•´åˆæ—¥å¿— (æµ·é©¬ä½“)\n"
            core_lines = [
                f"èƒŒæ™¯æ‘˜è¦: {bg_line}",
                f"å€™é€‰æ¡ç›®: {ltm_cnt if ltm_cnt is not None else '-'} | STM:{stm_sz if stm_sz is not None else '-'} | å†™å…¥:{len(written_any)} | çŠ¶æ€:{status_msg}",
                "å†™å…¥è¯¦æƒ…:",
                *write_lines,
                "---"
            ]
            block = header + "\n".join(core_lines)
            # æœºå™¨å¯è§£æ JSON è¿½åŠ ï¼ˆå¯ä¾›å‰ç«¯éšè—è§£æï¼‰
            summary_json = {
                "enabled": rep.get("enabled"),
                "error": rep.get("error"),
                "stm_size": rep.get("stm_size"),
                "ltm_candidate_count": rep.get("ltm_candidate_count"),
                "ltm_written": rep.get("ltm_written"),
                "has_background": bool(rep.get("background")),
            }
            json_marker = "<HIPPOCAMPUS>" + json.dumps(summary_json, ensure_ascii=False) + "</HIPPOCAMPUS>"
            return "\n\n" + block + "\n" + json_marker + "\n"
        assistant_output_buf = ""  # åœ¨å¼‚å¸¸è·¯å¾„ä¹Ÿå¯è®¿é—®
        # è‹¥ä»ç„¶æ˜¯å ä½æˆ–ç¼ºå¤±ï¼Œæå‰ç»™å‡ºé”™è¯¯é¿å…è°ƒç”¨ 404
        if not model or model.lower() in invalid_placeholders:
            err_payload = {
                "error": "model not configured (server fallback also empty)",
                "choices": [{"delta": {"content": "[ERROR] model not configured"}}]
            }
            yield f"data: {json.dumps(err_payload, ensure_ascii=False)}\n".encode("utf-8")
            # ä»ç„¶è¾“å‡ºæµ·é©¬ä½“çŠ¶æ€ï¼ˆåµŒå…¥å¼ï¼‰
            summary_tail = await finalize_and_build_hippocampus_summary()
            payload_tail = {"choices":[{"delta":{"content": summary_tail}}]}
            yield f"data: {json.dumps(payload_tail, ensure_ascii=False)}\n".encode('utf-8')
            yield b"data: [DONE]\n"
            return
        # å…è®¸ä¸€æ¬¡å†…è”å·¥å…·è°ƒç”¨æ‹¦æˆª
        inline_intercept_used = False
        async def run_stream(current_messages):
            logger.info("creating openai stream model=%s temp=%s max_tokens=%s", model, temperature, max_tokens)
            # åœ¨çœŸæ­£å‘èµ·æµå¼è°ƒç”¨å‰è¾“å‡ºç»™ä¸»æ¨¡å‹çš„æ¶ˆæ¯æ‘˜è¦ï¼Œä¾¿äºéªŒè¯æ¨¡å‹ä¸Šä¸‹æ–‡
            try:
                summary_list = []
                for i, mm in enumerate(current_messages):
                    if not isinstance(mm, dict):
                        continue
                    role = mm.get("role")
                    content = mm.get("content", "") or ""
                    c_preview = content.replace("\n", " ")
                    truncated = False
                    max_preview = 280
                    if len(c_preview) > max_preview:
                        c_preview = c_preview[:max_preview-1] + "â€¦"
                        truncated = True
                    meta_flags = []
                    if mm.get("_injected_raw"): meta_flags.append("raw")
                    if mm.get("_compressed_gap"): meta_flags.append("gap")
                    summary_list.append({
                        "i": i,
                        "role": role,
                        "len": len(content),
                        "preview": c_preview,
                        "truncated_preview": truncated,
                        "flags": ",".join(meta_flags) if meta_flags else ""
                    })
                logger.info("[model-input] total=%d messages: %s", len(current_messages), json.dumps(summary_list, ensure_ascii=False))
            except Exception as log_e:
                logger.warning("log model input failed: %s", log_e)
            return await client.chat.completions.create(  # type: ignore
                model=model,
                messages=current_messages,
                temperature=temperature,
                max_tokens=max_tokens,
                stream=True,
            )
        try:
            stream = await run_stream(msgs)
            got_any = False
            buffer_first = ""
            # assistant_output_buf å·²åœ¨å¤–å±‚å®šä¹‰
            async for chunk in stream:  # type: ignore
                if await request.is_disconnected():
                    logger.info("client disconnected during stream")
                    break
                try:
                    choice = chunk.choices[0]
                    piece = None
                    delta = getattr(choice, "delta", None)
                    if delta and getattr(delta, "content", None):
                        piece = delta.content
                    elif getattr(choice, "message", None) and getattr(choice.message, "content", None):
                        piece = choice.message.content
                    if piece is not None:
                        # ç´¯ç§¯å‰ 200 å­—èŠ‚æ£€æµ‹å†…è”å·¥å…· JSON æˆ–å‡½æ•°é£æ ¼è°ƒç”¨
                        if not got_any:
                            buffer_first += piece
                            snippet = buffer_first.strip()
                            inline_json = _try_parse_inline_tool(snippet) if len(snippet) < 600 else None
                            inline_func = None
                            if not inline_json:
                                # åŒ¹é… memory_store("...") æˆ– memory_recall("...")
                                mfunc = re.match(r"^(memory_store|memory_recall)\(\s*\"(.+?)\"\s*\)\s*$", snippet)
                                if mfunc:
                                    inline_func = (mfunc.group(1), {"text" if mfunc.group(1)=="memory_store" else "query": mfunc.group(2)})
                            if (inline_json or inline_func) and not inline_intercept_used:
                                inline_intercept_used = True
                                name, args = inline_json if inline_json else inline_func
                                # æ‰§è¡Œå·¥å…·
                                t0 = time.time()
                                if name == 'memory_store':
                                    result = await tool_memory_store(args.get('text',''))
                                else:
                                    result = await tool_memory_recall(args.get('query',''), args.get('top_k'))
                                dur = int((time.time()-t0)*1000)
                                executed_steps.append({"name": name, "arguments": args, "result_preview": json.dumps(result, ensure_ascii=False)[:300], "duration_ms": dur})
                                # å°†å·¥å…·ç»“æœå†™å…¥æ¶ˆæ¯å¹¶è¿½åŠ  system æŒ‡ä»¤å†é‡æ–°æµå¼å›ç­”
                                fake_tc_id = str(uuid.uuid4())
                                msgs.append({"role": "assistant", "content": "", "tool_calls": [{"id": fake_tc_id, "type": "function", "function": {"name": name, "arguments": json.dumps(args, ensure_ascii=False)}}]})
                                msgs.append({"role": "tool", "tool_call_id": fake_tc_id, "name": name, "content": json.dumps(result, ensure_ascii=False)})
                                msgs.append({"role": "system", "content": "ä¸Šè¿°å†…éƒ¨å·¥å…·å·²æ‰§è¡Œï¼Œè¯·ç”¨è‡ªç„¶è¯­è¨€ç»§ç»­å›å¤ç”¨æˆ·ï¼Œä¸è¦å†æ˜¾ç¤ºå·¥å…·è°ƒç”¨æˆ– JSONã€‚"})
                                # è¾“å‡ºè¿½åŠ çš„å·¥å…·æ­¥éª¤ markdownï¼ˆä»…æ–°å¢é‚£ä¸€æ­¥ï¼‰
                                step_md = format_tool_markdown(executed_steps[-1:])
                                for line in step_md.split('\n'):
                                    if not line:
                                        continue
                                    payload_md = {"choices":[{"delta":{"content": line + "\n"}}]}
                                    yield f"data: {json.dumps(payload_md, ensure_ascii=False)}\n".encode("utf-8")
                                # å¯åŠ¨ç¬¬äºŒé˜¶æ®µæµ
                                stream2 = await run_stream(msgs)
                                async for chunk2 in stream2:  # type: ignore
                                    if await request.is_disconnected():
                                        break
                                    try:
                                        ch2 = chunk2.choices[0]
                                        p2 = None
                                        d2 = getattr(ch2, "delta", None)
                                        if d2 and getattr(d2, "content", None):
                                            p2 = d2.content
                                        elif getattr(ch2, "message", None) and getattr(ch2.message, "content", None):
                                            p2 = ch2.message.content
                                        if p2:
                                            payload2 = {"choices":[{"delta":{"content": p2}}]}
                                            yield f"data: {json.dumps(payload2, ensure_ascii=False)}\n".encode("utf-8")
                                    except Exception as ie2:
                                        logger.warning("second stream parse error: %s", ie2)
                                        continue
                                # å†…è”å·¥å…·æ‹¦æˆªè·¯å¾„ï¼šä¹Ÿéœ€è¦åœ¨æœ«å°¾è¾“å‡ºæµ·é©¬ä½“æ—¥å¿—ï¼ˆä¸ä¸ä¸Šé¢ä¸»è·¯å¾„é‡å¤ï¼‰
                                summary_tail_inline = await finalize_and_build_hippocampus_summary()
                                payload_tail_inline = {"choices":[{"delta":{"content": summary_tail_inline}}]}
                                yield f"data: {json.dumps(payload_tail_inline, ensure_ascii=False)}\n".encode('utf-8')
                                yield b"data: [DONE]\n"
                                return
                        # æ­£å¸¸å†™å‡ºå½“å‰å—
                        payload = {"choices": [{"delta": {"content": piece}}]}
                        yield f"data: {json.dumps(payload, ensure_ascii=False)}\n".encode("utf-8")
                        got_any = True
                        assistant_output_buf += piece
                except Exception as inner_e:
                    logger.warning("stream chunk parse error: %s", inner_e)
                    continue
            if not got_any:
                logger.warning("openai stream returned no content pieces (empty response)")
            # ä¸»å›ç­”ç»“æŸåï¼šå°†æœ¬è½® assistant è¾“å‡ºå†™å…¥ raw_historyï¼ˆå½¢æˆå®Œæ•´åŒå‘å†å²ï¼‰
            try:
                if assistant_output_buf.strip():
                    cfg_all_local = load_config()
                    # ä¸å†åš max_raw æˆªæ–­ï¼Œç¡®ä¿å®Œæ•´åŸå§‹å¯¹è¯
                    sm_local = _load_short_mem()
                    sm_local.setdefault("raw_history", [])
                    sm_local['raw_history'].append({"role": "assistant", "content": assistant_output_buf.strip(), "ts": time.time()})
                    _save_short_mem(sm_local)
            except Exception as hist_e:
                logger.warning("append assistant history failed: %s", hist_e)
            # åœ¨ä¸»å›ç­”ç»“æŸåè¾“å‡ºæµ·é©¬ä½“è®°å¿†æ›´æ–°ï¼ˆæ— è®ºæ˜¯å¦å†™å…¥é•¿æœŸè®°å¿†ï¼Œåªè¦æ¨¡å—å¯ç”¨ï¼‰
            # æ€»æ˜¯è¾“å‡ºæµ·é©¬ä½“æ®µè½ï¼ˆå³ä½¿æœªå¯ç”¨/å¤±è´¥ï¼‰ï¼Œä»¥ä¾¿å‰ç«¯å¯æ„ŸçŸ¥
            # è‹¥æµ·é©¬ä½“ä»»åŠ¡ä»åœ¨è¿›è¡Œï¼Œè¿™é‡Œç­‰å¾…ï¼ˆä¸»å›ç­”å·²ç»“æŸï¼Œä¸å½±å“ç”¨æˆ·ä¸»ä½“ä½“éªŒï¼‰
            if 'hippocampus_task' in locals() and hippocampus_task is not None:
                if not hippocampus_task.done():
                    try:
                        hippocampus_report = await asyncio.wait_for(hippocampus_task, timeout=6.0)
                    except asyncio.TimeoutError:
                        hippocampus_report = {"enabled": False, "error": "hippocampus timeout"}
                    except Exception as he:
                        hippocampus_report = {"enabled": False, "error": str(he)}
                else:
                    try:
                        hippocampus_report = hippocampus_task.result()
                    except Exception as he2:
                        hippocampus_report = {"enabled": False, "error": str(he2)}
            # ç»Ÿä¸€è¾“å‡ºæµ·é©¬ä½“æ€»ç»“
            summary_tail_main = await finalize_and_build_hippocampus_summary()
            payload_tail_main = {"choices":[{"delta":{"content": summary_tail_main}}]}
            yield f"data: {json.dumps(payload_tail_main, ensure_ascii=False)}\n".encode('utf-8')
            yield b"data: [DONE]\n"
        except Exception as e:
            logger.error("openai streaming error: %s", e)
            err_payload = {"error": str(e), "choices": [{"delta": {"content": f"[ERROR] {str(e)}"}}]}
            yield f"data: {json.dumps(err_payload, ensure_ascii=False)}\n".encode("utf-8")
            summary_tail_err = await finalize_and_build_hippocampus_summary()
            payload_tail_err = {"choices":[{"delta":{"content": summary_tail_err}}]}
            yield f"data: {json.dumps(payload_tail_err, ensure_ascii=False)}\n".encode('utf-8')
            yield b"data: [DONE]\n"

    return StreamingResponse(gen(), media_type="text/event-stream")


@app.post("/memory/activate", response_model=ActivateResponse, summary="Stub activation retrieval")
def activate(req: ActivateRequest):
    # Placeholder: returns synthetic results for now
    fake = [
        MemoryChunk(id=f"stub-{i}", content=f"Stub memory chunk {i} for '{req.query}'", score=1.0 - i * 0.05)
        for i in range(req.top_k)
    ]
    return ActivateResponse(query=req.query, results=fake)


@app.post("/memory/write", response_model=WriteResponse, summary="Stub write endpoint")
def write(req: WriteRequest):
    ids: List[str] = []
    try:
        coll = get_chroma()
        # embed and add
        texts = req.chunks
        emb = asyncio.get_event_loop().run_until_complete(embed_texts(texts)) if asyncio.get_event_loop().is_running() is False else []
        # è‹¥åœ¨ event loop å†…è°ƒç”¨ï¼ˆä¾‹å¦‚ future æ‰©å±•ï¼‰ï¼Œç®€å•å…ˆç”¨é›¶å‘é‡é¿å…é˜»å¡
        if not emb:
            dim = get_embedding_dimension()
            emb = [[0.0]*dim for _ in texts]
        gen_ids = [str(uuid.uuid4()) for _ in texts]
        coll.add(ids=gen_ids, documents=texts, embeddings=emb)
        ids = gen_ids
    except Exception as e:
        logger.error("memory write failed: %s", e)
    return WriteResponse(written=len(req.chunks), ids=ids)


@app.get("/memory/debug/all", summary="List synthetic in-memory store (empty)")
def debug_all():
    # No real store yet
    return {"data": [], "note": "No real store implemented yet"}


# Convenience root
@app.get("/")
def root():
    return {"service": "limbic-memory-temp", "endpoints": ["/healthz", "/memory/activate", "/memory/write"], "note": "Temporary backend stub"}


# ---- Admin: reload config & cached clients ----
@app.post("/admin/reload")
def admin_reload():
    """æ¸…é™¤ LRU ç¼“å­˜, ä½¿ config.json æœ€æ–°å†…å®¹ï¼ˆåŒ…æ‹¬ system_prompt / api_key ç­‰ï¼‰ç«‹å³ç”Ÿæ•ˆã€‚

    å‰ç«¯æˆ–è¿ç»´åœ¨ä¿®æ”¹ backend/config.json åå¯è°ƒç”¨æœ¬ç«¯ç‚¹ï¼Œæ— éœ€æ•´ä½“é‡å¯è¿›ç¨‹ã€‚
    """
    try:
        load_config.cache_clear()
        get_chat_client.cache_clear()
        get_embedding_client.cache_clear()
        get_embedding_dimension.cache_clear()
        get_chroma.cache_clear()
        # è®¿é—®ä¸€æ¬¡ä»¥éªŒè¯é‡æ–°åŠ è½½æ˜¯å¦æˆåŠŸ
        _ = load_config()
        return {"status": "ok", "reloaded": True}
    except Exception as e:
        return JSONResponse({"status": "error", "message": str(e)}, status_code=500)
